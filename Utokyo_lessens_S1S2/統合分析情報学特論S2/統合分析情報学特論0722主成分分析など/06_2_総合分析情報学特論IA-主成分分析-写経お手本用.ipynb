{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 PCAのアルゴリズム\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 選択と抽出の比較\n",
    "### 12.4.1 特徴選択の実装\n",
    "\n",
    "まずは特徴選択の実装をしていきます｡今回も**iris**のデータセットを利用して､SVMによる分類を行います｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "#写経【1】\n",
    "# 必要なライブラリを読み込み\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【２】\n",
    "# irisデータのロード\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "#データとラベルに分ける\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【３】\n",
    "# データに含まれている特徴量を確認\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【４】\n",
    "# Xを確認(１０個)\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴選択による可視化\n",
    "# 4つの特徴量のうち､2つを選んで2次元にする\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 4つの特徴量の組み合わせを定義（合計6通り）\n",
    "# 0: sepal length\n",
    "# 1: sepal width\n",
    "# 2: petal length\n",
    "# 3: petal width\n",
    "pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "# 各組み合わせに対して散布図を表示\n",
    "# グラフのサイズを指定\n",
    "plt.figure(figsize=(12, 9))\n",
    "\n",
    "# グラフを特徴量のpairごとに表示\n",
    "# クラスラベルごとに色を分けてプロット\n",
    "# 赤: setosa \n",
    "# 青: versicolor\n",
    "# 緑: verginica\n",
    "for i, (p0, p1) in enumerate(pairs):\n",
    "    plt.subplot(2, 3, i + 1)\n",
    "    # クラスラベルごとに色分けしてプロット\n",
    "    for target, marker, color in zip(list(range(3)), \">ox\", \"rgb\"):\n",
    "        plt.scatter(X[iris.target == target, p0], X[iris.target == target, p1], marker=marker, c=color)\n",
    "    plt.xlabel(feature_names[p0])\n",
    "    plt.ylabel(feature_names[p1])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.2 特徴抽出の実装\n",
    "次は､**<font color='red'>PCA</font>**を用いて**特徴抽出**を行います｡**sklearn**の**decomposition**の中にある､**PCAクラス**を利用して行います｡\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【５】\n",
    "# データの標準化\n",
    "#StandardScalerのライブラリを読み込む\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#StandardScalerのインスタンスを生成\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 与えられた行列の各特徴量について､平均と標準偏差を算出\n",
    "scaler.fit(X)\n",
    "# Xを標準化した行列を生成\n",
    "X_std = scaler.fit_transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【６】\n",
    "# 主成分分析を実行\n",
    "#PCAを読み込み\n",
    "from sklearn.decomposition import PCA\n",
    "# PCAのインスタンスを生成し、主成分を4つまで取得\n",
    "pca = PCA(n_components=4)\n",
    "X_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【７】\n",
    "# 抽出した特徴量の値を出力(１０個)\n",
    "X_pca[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3 可視化による比較\n",
    "\n",
    "寄与率の大きい順に2つの主成分を選んで（$PC_1$ と $PC_2$）可視化をしてみます｡これが､**<font color='red'>irisのデータセットを要約した特徴空間</font>**になっています｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴抽出による可視化\n",
    "# PC0とPC1について散布図を表示\n",
    "plt.figure(figsize=(6, 6))\n",
    "for target, marker, color in zip(range(3), '>ox', 'rgb'):\n",
    "    plt.scatter(X_pca[iris.target==target, 0], X_pca[iris.target==target, 1], marker=marker, color=color)\n",
    "plt.xlabel('PC 0')\n",
    "plt.ylabel('PC 1')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCAの結果は以上のようになりました｡こちらが**irisのデータを要約した結果**です｡しかし､**特徴選択の場合のプロットと見比べてもそれほど大きな差異があるようには思えません**｡そこで､次節で**<font color='red'>より厳密な数値での指標をもとに効果を検証します</font>**｡\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 PCAの評価指標\n",
    "\n",
    "\n",
    "PCAの結果を定量的に評価するために､**代表的な評価指標**について学びます｡今回取り扱うのは､以下の3つの指標です｡\n",
    "\n",
    "1. 寄与率\n",
    "2. 累積寄与率\n",
    "3. 因子負荷量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.1 寄与率\n",
    "\n",
    "寄与率とは､**<font color='red'>ひとつの主成分がデータ全体の情報をどれだけの割合拾えているか</font>**を表す指標です｡数式的には､**<font color='red'>各主成分の持つ固有値の比率</font>**を表します｡固有値とは､線形代数に登場する $\\lambda$ などの文字で表される指標です｡主成分分析においては､**この固有値が各主成分の持つ情報量を表す指標として利用できます**｡定義より､寄与率は**0~1**の値を取り､**その総和は1になります**｡**<font color='red'>寄与率が高ければ高いほど､その主成分はデータを効率よく要約する良い主成分であるといえます</font>**｡\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_17.1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【８】\n",
    "# 寄与率を出力\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.2 累積寄与率\n",
    "\n",
    "累積寄与率とは､その名の通り**<font color='red'>寄与率の累積値</font>**であり､**<font color='red'>これまでに採用した主成分すべてで拾えた情報が合計でどの程度になるか</font>**を表す指標になります｡また､寄与率と累積寄与率は以下のような図で表現することができます｡**各主成分は互いに直行しているため､互いの拾う情報に重複がないところがポイントです**｡\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_17.2.png)\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_17.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【９】\n",
    "# 累積寄与率を出力\n",
    "print(np.cumsum(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて､80:20の法則によれば､**第3､第4主成分はほとんど分析に貢献しないものとされます**｡それが本当なのか､可視化をして確認してみましょう｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴抽出による可視化\n",
    "# PC3とPC4について散布図を表示\n",
    "plt.figure(figsize=(6, 6))\n",
    "for target, marker, color in zip(range(3), '>ox', 'rgb'):\n",
    "    plt.scatter(X_pca[iris.target==target, 2], X_pca[iris.target==target, 3], marker=marker, color=color)\n",
    "plt.xlabel('PC 2')\n",
    "plt.ylabel('PC 3')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下に**上位の主成分との比較画像**を用意しました｡出力した結果､下位の主成分は目で見ても明らかに分類に貢献しなさそうなことがわかります｡それだけ､**上位の主成分が情報量を効率よく吸収していることがわかります**｡irisデータは4次元と低次元なデータなのでそれほど心配はいりませんが､**<font color='red'>特徴量が多いデータになるほど次元削減を行うことの利益は大きくなります</font>**｡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.3 因子負荷量\n",
    "\n",
    "因子負荷量（主成分負荷量）とは､**<font color='red'>主成分がもとの特徴量をどのように合成して作られたものなのかを表す指標</font>**です｡それぞれの主成分に対して､**もとの各特徴量との相関係数にあたる数値**が割り当てられており､**-1~+1**の値を取ります｡因子負荷量を参照することで､**<font color='red'>導かれた主成分の意味付けを行うことができます</font>**｡ひとつの主成分が､**どのような特徴量と相関が強いのか（軸が同じ向きや逆の向きになっているのか）**､あるいは**どのような特徴量とは相関がないのか（軸が直行しているかどうか）**に着目することで､主成分の表す意味が浮かび上がってきます｡\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_20.png)\n",
    "\n",
    "数式的には､**主成分に含まれる各特徴量の係数 $h_j$ に､その主成分の固有値 $l_i$ の平方根  $\\sqrt{l_i}$ を掛け算して表現されます**｡重要なのは､ $h_j$ を利用して定義されていることから､**<font color='red'>もともとの特徴量との相関が考慮されていること</font>**が確認できることです｡\n",
    "\n",
    "![](https://ai-std-contents.azureedge.net/image/ml12_21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにして主成分に意味付けを行うことができれば､各サンプルを従来の特徴量の値ではなく**<font color='red'>各主成分 $PC_i$ の値（主成分スコア）を用いて評価することができます</font>**｡つまりは､**｢この学生は地頭がどの程度良いか？どれだけ理系寄りか？｣**であったり､**｢どれだけ体格ががっちりしているか？どれだけスレンダーか？｣**という評価の仕方が出来るのです｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1０】\n",
    "# 主成分の係数h_jを出力\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1１】\n",
    "# 固有値lのルートを取る\n",
    "np.sqrt(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1２】\n",
    "# 因子負荷量を出力\n",
    "pca.components_ * np.sqrt(pca.explained_variance_)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6 分類精度への貢献\n",
    "\n",
    "これまでに利用してきた特徴選択とPCAによる特徴抽出を比較して､**<font color='red'>分類精度がどれだけ上がるのか</font>**を確認します｡両者において**交差検証**を行います｡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.1 特徴選択の場合\n",
    "\n",
    "まずは特徴選択です｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1３】\n",
    "# PCA前のデータを確認\n",
    "# 標準化をしたとこまでのデータを利用\n",
    "X_std[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1４】\n",
    "# 特徴量選択のために､特徴量を確認\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1５】\n",
    "# 特徴選択(１０個)\n",
    "X_std[:, 0:2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1６】\n",
    "# SVMで分類\n",
    "#SCMのクラスを読み込む\n",
    "from sklearn.svm import SVC\n",
    "#モデルを評価するクラスを読み込む\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#インスタンスを生成する\n",
    "scores_1 = cross_val_score(SVC(), X_std[:, [0,2]], y, cv=5)\n",
    "#平均値を求める\n",
    "scores_1.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.2 特徴抽出の場合\n",
    "\n",
    "次に､**特徴抽出**を行って実行します｡こちらも同様に2つの特徴量を利用して行います｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【17】\n",
    "# PCA後のデータを確認(１０個)\n",
    "\n",
    "X_pca[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1８】\n",
    "# 第2主成分までを選択(１０個)\n",
    "\n",
    "X_pca[:, 0:2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【1９】\n",
    "# SVMで分類\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_2 = cross_val_score(SVC(), X_pca[:, [0,2]], y, cv=5)\n",
    "scores_2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "両者を比較すると､特徴抽出の方が2%ほど性能が高いことがわかります｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#写経【２０】\n",
    "# 両者の精度を比較\n",
    "#特徴選択\n",
    "print('特徴選択: {}'.format(scores_1.mean()))\n",
    "#特徴抽出\n",
    "print('特徴抽出: {}'.format(scores_2.mean()))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
