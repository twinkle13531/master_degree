{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピック分析1: Bag-of-Words (BoW)による文書のベクトル表現\n",
    "\n",
    "BoWとは、その文書が、どのような単語の集合から構成されているかだけに注目して、その文書の特徴をベクトル表現に変換する手法です。   \n",
    "つまりBoWでは単語の並びは考慮しません。\n",
    "\n",
    "## 1. 文書間の類似度\n",
    "\n",
    "下のセルでtextに代入された８文のそれぞれが（非常に短いですが）一つの文書と考えて、それぞれの文書の文書ベクトルを算出しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙サイズ: 15\n",
      "---- 語彙リストを出力する。数字はその単語のID ----\n",
      "[('。', 0), ('が', 1), ('だ', 2), ('で', 3), ('は', 4), ('を', 5), ('今日', 6), ('明日', 7), ('晴天', 8), ('本', 9), ('私', 10), ('読む', 11), ('降る', 12), ('雨', 13), ('！', 14)]\n",
      "---- 各文書の文書ベクトル ----\n",
      "私 は 本 を 読む 。 :\t [1 0 0 0 1 1 0 0 0 1 1 1 0 0 0]\n",
      "今日 は 晴天 だ 。 :\t [1 0 1 0 1 0 1 0 1 0 0 0 0 0 0]\n",
      "私 は 私 だ 。 :\t [1 0 1 0 1 0 0 0 0 0 2 0 0 0 0]\n",
      "本 は 本 で 本 だ 。 :\t [1 0 1 1 1 0 0 0 0 3 0 0 0 0 0]\n",
      "私 は 本 を 読む 。 :\t [1 0 0 0 1 1 0 0 0 1 1 1 0 0 0]\n",
      "私 本 を 読む 。 :\t [1 0 0 0 0 1 0 0 0 1 1 1 0 0 0]\n",
      "私 は 本 を 読む 。 本 を 私 は 読む 。 :\t [2 0 0 0 2 2 0 0 0 2 2 2 0 0 0]\n",
      "明日 雨 が 降る ！ :\t [0 1 0 0 0 0 0 1 0 0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# たった5, 6単語からなる文書ですが、以下の各列を1つの文書と考えましょう\n",
    "text = ['私 は 本 を 読む 。',\n",
    "        '今日 は 晴天 だ 。',\n",
    "        '私 は 私 だ 。',\n",
    "        '本 は 本 で 本 だ 。',\n",
    "        '私 は 本 を 読む 。',\n",
    "        '私 本 を 読む 。',\n",
    "        '私 は 本 を 読む 。 本 を 私 は 読む 。',\n",
    "        '明日 雨 が 降る ！']\n",
    "\n",
    "# BoWを作成する準備\n",
    "# token_patternは、文書中で1単語がどう書かれているかのパターンを指す\n",
    "# ここではスペース以外の文字が1文字以上続いたものを1単語とする（スペースが来たらそこで単語終わり）\n",
    "count = CountVectorizer(token_pattern=r'[^\\s]+')\n",
    "# 文書からbowを取り出す\n",
    "bow = count.fit_transform(text)\n",
    "\n",
    "# 語彙サイズ\n",
    "print('語彙サイズ:', len(count.get_feature_names()))\n",
    " \n",
    "print('---- 語彙リストを出力する。数字はその単語のID ----')\n",
    "print(sorted(count.vocabulary_.items(), key=lambda x:x[1]))\n",
    "# 語彙サイズは15（0～14)であることから、文書ベクトルの次元は15次元となる\n",
    "\n",
    "print('---- 各文書の文書ベクトル ----')\n",
    "# 例えば1文目　'私 は 本 を 読む 。'は「私」「は」「本」「を」「読む」「。」の6個の単語から構成されるため、\n",
    "# そのそれぞれの単語のIDの値が1になり、文書に現れていないIDの値は0になっている\n",
    "vec = bow.toarray()\n",
    "\n",
    "for i in range(len(text)):\n",
    "    print(text[i], ':\\t', vec[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この８つの文書の語彙は合わせて15種類(IDは0～14)なので、各文書は15次元の文書ベクトルに変換されます。   \n",
    "ある文書の文書ベクトルとは、その文書中に各語彙が何回現れるかを数えてベクトル化したものです。   \n",
    "各文書と、各語彙の出現回数は以下の表のようになります。\n",
    "\n",
    "| 文書 | 0=。| 1=が | 2=だ | 3=で | 4=は | 5=を | 6=今日 | 7=明日 | 8=晴天 | 9=本 | 10=私 | 11=読む | 12=降る | 13=雨 | 14=！ |\n",
    "| -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- |\n",
    "| 「私 は 本 を 読む 。」 | 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0|\n",
    "| 「今日 は 晴天 だ 。」 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0|\n",
    "| 「私 は 私 だ 。」 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0|\n",
    "| 「本は 本 で 本 だ 。」 | 1 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0|\n",
    "| 「私 は 本 を 読む 。」| 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0|\n",
    "| 「私 本 を 読む 。」| 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0|\n",
    "| 「私 は 本 を 読む 。 本 を 私 は 読む 。」| 2 | 0 | 0 | 0 | 2 | 2 | 0 | 0 | 0 | 2 | 2 | 2 | 0 | 0 | 0 |\n",
    "| 「明日 雨 が 降る ！」 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 1 | 1|\n",
    "\n",
    "「私 は 私 だ 。」\tは「私」が2回でているので、「10=私」の欄の値が2になっていますね。   \n",
    "また、「私 は 本 を 読む 。」と「私 本 を 読む 。」の文書ベクトルを比べると、「4=は」のみが異なり、\n",
    "それ以外は全く同じであることが分かります。\n",
    "「私 は 本 を 読む 。」に比べて「私 は 本 を 読む 。 本 を 私 は 読む 。」はすべての単語が2回ずつ現れているので、ベクトルの要素の値はすべて倍になっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. コサイン距離によるベクトル間の類似度\n",
    "\n",
    "文書間の類似度は、この文書ベクトル同士がどれだけ似通っているかで判断することができそうです。   \n",
    "ベクトル間の類似度の尺度はいろいろありますが、中でもよくつかわれるのがコサイン類似度です。   \n",
    "これは、二つのベクトルのなす角を$\\theta$とすると、$\\cos (\\theta)$となります。   \n",
    "通常、ベクトル間のコサイン類似度は$-1$から$1$までの値をとりますが、文書ベクトルは通常、正の要素しか持たないのでベクトルの向きが逆になることはないことから、文書ベクトル間のコサイン類似度は$0$から$1$までとなります。\n",
    "\n",
    "ベクトル$V_1$とベクトル$V_2$の類似度$\\mbox{cos_sim}$は以下のように計算できます。\n",
    "$$\\mbox{cos_sim}(V_1, V_2) = \\frac{V_1 \\cdot V_2}{|V_1||V_2|}$$\n",
    "\n",
    "ベクトルの内積で表しているのでわかりづらいかもしれませんが、高校で習った式と同じです。   \n",
    "二つの直線$ax+by=0$，$cx+dy=0$のなす角$\\theta$を求めるには、  \n",
    "$$\\cos (\\theta) = \\frac{|ac+bd|}{\\sqrt{a^2 + b^2}\\sqrt{c^2 + d^2}}$$\n",
    "でしたね。   \n",
    "$ax+by=0$の法線ベクトルは$(a,b)$であること考慮すると、これはベクトル$(a, b)$とベクトル$(c, d)$のなす角を計算していることになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを使って、文書間のコサイン類似度を計算してみましょう。   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "「 私 は 本 を 読む 。 」と「 今日 は 晴天 だ 。 」の類似度:\t 0.365\n",
      "「 私 は 本 を 読む 。 」と「 私 は 私 だ 。 」の類似度:\t 0.617\n",
      "「 私 は 本 を 読む 。 」と「 本 は 本 で 本 だ 。 」の類似度:\t 0.566\n",
      "「 私 は 本 を 読む 。 」と「 私 は 本 を 読む 。 」の類似度:\t 1.0\n",
      "「 私 は 本 を 読む 。 」と「 私 本 を 読む 。 」の類似度:\t 0.913\n",
      "「 私 は 本 を 読む 。 」と「 私 は 本 を 読む 。 本 を 私 は 読む 。 」の類似度:\t 1.0\n",
      "「 私 は 本 を 読む 。 」と「 明日 雨 が 降る ！ 」の類似度:\t 0.0\n"
     ]
    }
   ],
   "source": [
    "# 1文目と2～4文目はどの程度近いかを、文書ベクトルのコサイン類似度で評価してみよう\n",
    "for i in range(1, len(text)):\n",
    "    sim = cos_sim(vec[0], vec[i])\n",
    "    print('「', text[0], '」と「', text[i], '」の類似度:\\t', round(sim, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「 私 は 本 を 読む 。 」と「 私 本 を 読む 。 」は1単語違うだけなので、0.913という高い値を示しています。   \n",
    "一方で「 私 は 本 を 読む 。 」と「 今日 は 晴天 だ 。 」では、一致している単語が「は」と「。」しかありませんので、0.365と低い値を示します。   \n",
    "「 私 は 本 を 読む 。 」と「 私 は 本 を 読む 。 」は完全に同じ文書なので類似度は1.0となります。  \n",
    "また、「 私 は 本 を 読む 。 」と「 私 は 本 を 読む 。 本 を 私 は 読む 。 」とは、同じ文書ではありませんが、\n",
    "後者のベクトルの長さが前者の2倍になっただけで、ベクトルのなす角は0ですから、類似度はやはり1.0となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 小説間の類似度\n",
    "\n",
    "今度はもう少し長い文書の類似度を計算してみましょう。\n",
    "\n",
    "宮沢賢治の「銀河鉄道の夜」「風の又三郎」と太宰治の「人間失格」の3作品について、\n",
    "お互いがどの程度近いかを文書ベクトルのコサイン類似度で評価してみましょう。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "titles = ['銀河鉄道の夜', '風の又三郎', '人間失格']\n",
    "\n",
    "# 以下に3作品の分かち書き文が入っています\n",
    "files = ['texts/gingatetsudonoyoru_wakati.txt', 'texts/kazenomatasaburo_wakati.txt', 'texts/ningenshikkaku_wakati.txt']\n",
    "\n",
    "# 3つの文書を読み込んで一つのリスト`wakati_all`にまとめます\n",
    "wakati_all = []\n",
    "for i in range(len(files)):\n",
    "    with open(files[i], 'r', encoding='utf-8') as f:\n",
    "        wakati_all.append(f.read().replace('\\n', ' '))\n",
    "\n",
    "# BoWを作成する準備\n",
    "# token_patternは、文書中で1単語がどう書かれているかのパターンを指す\n",
    "# ここではスペース以外の文字が1文字以上続いたものを1単語とする（スペースが来たらそこで単語終わり）\n",
    "novel_count = CountVectorizer(token_pattern=r'[^\\s]+')\n",
    "# 文書からbowを取り出す\n",
    "novel_bow = novel_count.fit_transform(wakati_all)\n",
    "novel_vec = novel_bow.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで、`novel_vec`に各小説の文書ベクトルが代入されました。   \n",
    "それでは、3つの小説が互いにどの程度近いのかを、類似度行列として出力してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "語彙サイズ: 7721\n",
      "「銀河鉄道の夜」,「風の又三郎」,「人間失格」\n",
      "[[1.         0.9402969  0.87744071]\n",
      " [0.9402969  1.         0.81866464]\n",
      " [0.87744071 0.81866464 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 語彙リスト\n",
    "#print('語彙リスト:', novel_count.get_feature_names())\n",
    " \n",
    "# 語彙サイズ\n",
    "print('語彙サイズ:', len(novel_count.get_feature_names()))\n",
    " \n",
    "# 「銀河鉄道の夜」「風の又三郎」「人間失格」のconfusion matrixを出力してみましょう\n",
    "novel_sim = []\n",
    "for i in range(len(files)):\n",
    "    novel_sim.append([])\n",
    "    for j in range(len(files)):  \n",
    "        novel_sim[i].append(cos_sim(novel_vec[i], novel_vec[j]))\n",
    "print('「銀河鉄道の夜」,「風の又三郎」,「人間失格」')\n",
    "print(np.array(novel_sim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを見ると、「銀河鉄道の夜」と「風の又三郎」は0.94と似ていますが、「銀河鉄道の夜」と「人間失格」とは0.88と相対的に似ていないことが分かります。   \n",
    "「人間失格」には、「風の又三郎」よりも「銀河鉄道の夜」のほうが似ているようですね。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本課題の作品データは[青空文庫](https://www.aozora.gr.jp/index.html)のものを使用しています。  \n",
    "ただし、ルビや入力者注、アクセント分解された欧文や編者による注記等は削除しました。   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
