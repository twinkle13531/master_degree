{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5回課題：tf-idfを使って「銀河鉄道の夜」の重要語を抽出しよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"2019-07-15 12.50.02.png\"><br/>\n",
    "<img src=\"2019-07-15 12.49.37.png\"><br/>\n",
    "<img src=\"2019-07-15 12.49.11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 入力データの説明\n",
    "\n",
    "`texts/novels_miyazawa_wakati.json`には、宮沢賢治の全小説の分かち書き文がjson形式で記録されています。   \n",
    "記録形式は以下の通りです。\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        \"author\": \"宮沢賢治\",\n",
    "        \"title\": \"『春と修羅』\",\n",
    "        \"text\": \"目次 \\n『 春 と 修羅 』 \\n春 と...(本文続く）\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "`title`が`銀河鉄道の夜`であるような要素の`text`の値には「銀河鉄道の夜」の本文の分かち書き文が納められています。\n",
    "\n",
    "## 課題１：tf-idfモデルの学習\n",
    "\n",
    "宮沢賢治の全作品を使って、各作品を1文書としたときのtf-idfを学習してください。   \n",
    "tf-idfの学習には、`sklearn.feature_extraction.text`の`TfidfVectorizer`を使ってください。   \n",
    "ここで、TfidfVectorizerのパラメータは以下のようにしてください。\n",
    "-    max_features=1000 (語彙サイズは最大1000個)\n",
    "-    max_df=5　（5つ以下の小説までに現れる語彙）\n",
    "-    min_df=3　（3つ以上の小説に現れる語彙）\n",
    "\n",
    "## 課題２：「銀河鉄道の夜」の重要語の抽出\n",
    "\n",
    "小説「銀河鉄道の夜」のtf-idf値が大きい単語上位10単語とそのtf-idf値を、   \n",
    "各行にカンマ区切りで単語とtf-idf値が並んだ形式で、ex5-tfidf.txtという名前のファイルに出力してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヒント\n",
    "\n",
    "`TfidfVectorizer`には、各要素が1つの小説の本文全体（単語が半角空文字で区切られた分かち書き文）であるようなリストを渡します。   \n",
    "今回入力とするjsonファイルには249作品が納められているので、`TfidfVectorizer`に渡す行列は249個の要素を持つのベクトルということになります。   \n",
    "このようなリストは以下のプログラムで作成することができます。\n",
    "\n",
    "以下のプログラムでは、\n",
    "\n",
    "1. `texts/novels_miyazawa_wakati.json`の本文（jsonファイルを辞書として読み込んだ時の各要素のキーが`text`）を読み込み\n",
    "'\\n'を空白に置換（replace('\\n', ' '))した後で、`wakati`という名前のリストに追加(append)しています。   \n",
    "つまり、`wakati`の$i$番目の要素は、$i$番目に登録されている小説の本文（全文の分かち書き文）というわけです。   \n",
    "`novels_miyazawa_wakati.json`には249作品が納められているので、このリストは249次元となります。   \n",
    "\n",
    "2. また同時に、`novels_miyazawa_wakati.json`において、ある作品名の小説が何番目に登録されているかを返す辞書`title2ID`を生成しています。   \n",
    "つまり、`title2ID['作品名'] = [登場順]`です。   \n",
    "たとえば「銀河鉄道の夜」は0番目から数えて225番目に現れるので、  \n",
    "    `title2ID['銀河鉄道の夜'] = 225`  \n",
    "となります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "小説の数: 249\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "### このセルは変更しないでください！！！ ###\n",
    "############################################\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "file = 'texts/novels_miyazawa_wakati.json'\n",
    "\n",
    "with open(file, 'r', encoding='utf-8') as fi:\n",
    "    novels = json.load(fi)\n",
    "\n",
    "print('小説の数:', len(novels))#249\n",
    "\n",
    "wakati = [] #novels_miyazawa_wakati.jsonには249作品が納められているので、このリストは249次元となります。\n",
    "num = 0\n",
    "title2ID = {}\n",
    "#text = jsonファイルを辞書として読み込んだ時の各要素のキー\n",
    "for novel in novels:\n",
    "    wakati.append(novel['text'].replace('\\n',' ')) #i番目に登録されている小説の本文（全文の分かち書き文）\n",
    "    title2ID[novel['title']] = num#ある作品名の小説が何番目に登録されているかを返す辞書title2IDを生成\n",
    "    num += 1\n",
    "    \n",
    "print(title2ID['銀河鉄道の夜']) #225\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題１：tf-idfモデルの学習\n",
    "\n",
    "以下にtf-idfを学習するプログラムを書いてください。   \n",
    "上のヒントを使ってもいいし使わなくてもいいです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_features=1000 (語彙サイズは最大1000個)`\n",
    "という条件が指定されていますから、\n",
    "`vectorizer = TfidfVectorizer(max_features=1000, max_df=5, min_df=3)`\n",
    "となります。\n",
    "また、学習をする\n",
    "`X = vectorizer.fit_transform(wakati)`\n",
    "の行が必要です。\n",
    "\n",
    "後半は以下のようにすればOKです。\n",
    "`\n",
    "    feature_names = vectorizer.get_feature_names() `\n",
    "1000種類の単語のリストを獲得します\n",
    "単語とそのtf-idf値の対を辞書として登録\n",
    "`pair = dict(zip(feature_names, X[title2ID['銀河鉄道の夜'],:].toarray()[0]))`\n",
    "tf-idfの高い順にソートして、単語とtf-idfの対をタプルとしてリスト化し、辞書に代入する\n",
    "`dic = [(x, pair[x]) for x in sorted(pair, key=lambda x:-pair[x])]`\n",
    "\n",
    "結果の出力には以下を使ってください。\n",
    "`with open('ex5-tfidf.txt', 'w', encoding='utf-8') as f:`\n",
    "`for d in dic[:10]:`\n",
    "`f.write(d[0]+','+str(round(d[1],3))+'\\n')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfVectorizerには、各要素が1つの小説の本文全体（単語が半角空文字で区切られた分かち書き文）であるようなリストを渡します。\n",
    "#今回入力とするjsonファイルには249作品が納められているので、TfidfVectorizerに渡す行列は249個の要素を持つのベクトルということになります。\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000, max_df=5, min_df=3)\n",
    "X = vectorizer.fit_transform(wakati)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()#1000種類の単語のリストを獲得\n",
    "pair = dict(zip(feature_names, X[title2ID['銀河鉄道の夜'],:].toarray()[0]))\n",
    "dic = [(x, pair[x]) for x in sorted(pair, key=lambda x:-pair[x])]\n",
    "\n",
    "with open('ex5-tfidf.txt', 'w', encoding='utf-8') as f:\n",
    "    for d in dic[:10]:\n",
    "        f.write(d[0]+','+str(round(d[1],3))+'\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題２：「銀河鉄道の夜」の重要語の抽出\n",
    "\n",
    "小説「銀河鉄道の夜」のtf-idf値が大きい単語上位10単語とそのtf-idf値を、   \n",
    "**各行にカンマ区切りで単語とtf-idf値が並んだ形式で、`ex5-tfidf.txt`という名前のファイルに出力**してください。\n",
    "\n",
    "ex5-tfidf.txt`は以下のようにります。\n",
    "```\n",
    "神さま,0.447\n",
    "切符,0.289\n",
    "牛乳,0.263\n",
    "ぱいに,0.237\n",
    "十字架,0.22\n",
    "向う岸,0.22\n",
    "白鳥,0.22\n",
    "さそり,0.202\n",
    "烏瓜,0.192\n",
    "もろこし,0.165\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\n",
    "`TopicAnalysis2.ipynb`の内容を使えばできるはずです。\n",
    "ただし、`TopicAnalysis2.ipynb`では文書数(Categories)は6個でしたが、課題では文書数len(novels)は249（0～248)です。\n",
    "Xには249個の小説それぞれの語彙1000単語のtf-idf値が並んでいるので、「銀河鉄道の夜」（225番目）のtf-idf値は\n",
    "     \n",
    "`X[225,:].toarray()[0]`\n",
    "\n",
    "  でとってくることができます。<br/>\n",
    "`pair = dict(zip(feature_names, X[225,:].toarray()[0]))`\n",
    "\n",
    "   とすることで、「銀河鉄道の夜」における、各語彙とそのtf-idf値のペアを生成することができます。\n",
    "あとは、pairのうちtf-idfが高いものから順に10個をとってくればOKです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dic = {}\n",
    "#単語とそのtf-idf値の対を辞書として登録\n",
    "#語彙1000単語のtf-idf値が並んでいる\n",
    "pair = dict(zip(feature_names, X[225,:].toarray()[0]))#各語彙とそのtf-idf値のペアを生成\n",
    "# tf-idfの高い順にソートして、単語とtf-idfの対をタプルとしてリスト化し、辞書に代入する\n",
    "dic.update([(x, pair[x]) for x in sorted(pair, key=lambda x:-pair[x])])\n",
    "result = {k:d[k] for k in list(d)[:3]}\n",
    "#result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_dic = {k:dic[k] for k in list(dic)[:10]}\n",
    "#esult_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神さま,0.4102\n",
      "切符,0.2654\n",
      "牛乳,0.2413\n",
      "ぱいに,0.2172\n",
      "十字架,0.2018\n",
      "向う岸,0.2018\n",
      "白鳥,0.2018\n",
      "さそり,0.1859\n",
      "烏瓜,0.1766\n",
      "もろこし,0.1514\n"
     ]
    }
   ],
   "source": [
    "for k in list(dic)[:10]:\n",
    "    print(k+',''{:.4g}'.format(dic[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
