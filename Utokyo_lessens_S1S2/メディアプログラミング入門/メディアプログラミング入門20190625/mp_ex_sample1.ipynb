{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMで自然言語処理\n",
    "- https://www.atmarkit.co.jp/ait/articles/1801/30/news139.html\n",
    "- 自然言語をディープラーニングで扱う場合、何らかの方法で単語をベクトルデータに変換する必要がある。しゃれた言い方では、n次元実数空間への埋め込みであるが、要はn個の実数の組に対応させることである。とにかく、数字に変換しないと始まらない。\n",
    "- one-hotベクトルへの変換...自然言語のベクトル化手法のうち最もかんたんな方法。語彙数がNであるとき、各単語に0からN-1までのインデックスiを振る。各単語に対し、i次元の値が1で、それ以外の値が0のベクトル（1つの次元だけが1で他が0のため、one-hotベクトルと呼ばれる）に対応させると、確かにN次元実数空間への埋め込みが実現できる。\n",
    "- kerasには「Embedding」レイヤーというものが標準で用意されており、指定された次元への埋め込みベクトルを生成してくれる。\n",
    "- word2vec...単語間の関連性を対応するベクトル間演算（足し引き）で表現できるようにしようというもの、word to vector、自然言語のベクトル化手法のうち語彙数より少ない次元Embedding数のベクトル化手法。CBOW（Continuous Bag-of-Words Model）とContinuous Skip-gram Model（以下、skip-gramと表記）の2種類の手法がある。\n",
    " - CBOW...前後の単語からターゲットの単語を予測しようというもの\n",
    " - skip-gram...特定の単語が与えられたときに、その前後の単語を当てようというもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy.random as nr\n",
    "import sys\n",
    "import h5py\n",
    "import keras\n",
    "import math\n",
    "\n",
    "\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Masking\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.initializers import uniform\n",
    "from keras.initializers import orthogonal\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import regularizers\n",
    "from keras.constraints import maxnorm, non_neg\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rampo_separate.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d97759ecd39a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 元データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rampo_separate.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rampo_separate2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rampo_separate3.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rampo_separate.csv'"
     ]
    }
   ],
   "source": [
    "# 元データ\n",
    "df1 = csv.reader(open('rampo_separate.csv', 'r'))\n",
    "df2 = csv.reader(open('rampo_separate2.csv', 'r'))\n",
    "df3 = csv.reader(open('rampo_separate3.csv', 'r'))\n",
    "\n",
    "\n",
    "data1 = [ v for v in df1]\n",
    "data2 = [ v for v in df2]\n",
    "data3 = [ v for v in df3]\n",
    "\n",
    "mat1 = np.array(data1)\n",
    "mat2 = np.array(data2)\n",
    "mat3 = np.array(data3)\n",
    "\n",
    "mat = np.r_[mat1[:,0], mat2[:,0], mat3[:,0]]\n",
    "print(mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "words = sorted(list(set(mat)))\n",
    "cnt = np.zeros(len(words))\n",
    "\n",
    "print('total words:', len(words))\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))  # 単語をキーにインデックス検索\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))  # インデックスをキーに単語を検索\n",
    "\n",
    "# 単語の出現数をカウント\n",
    "for j in range (0, len(mat)):\n",
    "    cnt[word_indices[mat[j]]] += 1\n",
    "\n",
    "# 出現頻度の少ない単語を「UNK」で置き換え\n",
    "words_unk = []                           # 未知語一覧\n",
    "\n",
    "for k in range(0, len(words)):\n",
    "    if cnt[k] <= 3 :\n",
    "        words_unk.append(words[k])\n",
    "        words[k] = 'UNK'\n",
    "\n",
    "print('低頻度語数:', len(words_unk))    # words_unkはunkに変換された単語のリスト\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "print('total words:', len(words))\n",
    "word_indices = dict((w, i) for i, w in enumerate(words))  # 単語をキーにインデックス検索\n",
    "indices_word = dict((i, w) for i, w in enumerate(words))  # インデックスをキーに単語を検索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "maxlen = 10                   # 前後の語数\n",
    "\n",
    "mat_urtext = np.zeros((len(mat), 1), dtype=int)\n",
    "for i in range(0, len(mat)):\n",
    "    if mat[i] in word_indices :        \n",
    "        mat_urtext[i, 0] = word_indices[mat[i]]\n",
    "    else:\n",
    "        mat_urtext[i, 0] = word_indices['UNK']\n",
    "\n",
    "print(mat_urtext.shape)\n",
    "\n",
    "len_seq = len(mat_urtext) - maxlen\n",
    "data = []\n",
    "target = []\n",
    "for i in range(maxlen, len_seq):\n",
    "    data.append(mat_urtext[i])\n",
    "    target.extend(mat_urtext[i-maxlen:i])\n",
    "    target.extend(mat_urtext[i+1:i+1+maxlen])\n",
    "\n",
    "x_train = np.array(data).reshape(len(data), 1)\n",
    "t_train = np.array(target).reshape(len(data), maxlen*2)\n",
    "\n",
    "z = zip(x_train, t_train)\n",
    "nr.seed(12345)\n",
    "nr.shuffle(z)                 # シャッフル\n",
    "x_train, t_train = zip(*z)\n",
    "\n",
    "x_train = np.array(x_train).reshape(len(data), 1)\n",
    "t_train = np.array(t_train).reshape(len(data), maxlen*2)\n",
    "\n",
    "print(x_train.shape, t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネットワーク本体\n",
    "\n",
    "\n",
    "class Prediction :\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.input_dim, self.output_dim, input_length=1, embeddings_initializer=uniform(seed=20170719)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.input_dim, use_bias=False, kernel_initializer=glorot_uniform(seed=20170719)))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=\"RMSprop\", metrics=['categorical_accuracy'])\n",
    "        print('#2')\n",
    "        return model\n",
    "   \n",
    "  # 学習\n",
    "    def train(self, x_train, t_train,batch_size, epochs, maxlen, emb_param) :\n",
    "        early_stopping = EarlyStopping(monitor='categorical_accuracy', patience=1, verbose=1)\n",
    "        print ('#1', t_train.shape)\n",
    "        model = self.create_model()\n",
    "        #model.load_weights(emb_param)    # 埋め込みパラメーターセット。ファイルをロードして学習を再開したいときに有効にする\n",
    "        print ('#3')\n",
    "        model.fit(x_train, t_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "              shuffle=True, callbacks=[early_stopping], validation_split=0.0)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#メイン処理\n",
    "\n",
    "\n",
    "vec_dim = 100\n",
    "epochs = 10\n",
    "batch_size = 200\n",
    "input_dim = len(words)\n",
    "output_dim = vec_dim\n",
    "\n",
    "emb_param = 'param_skip_gram_2_1.hdf5'    # 学習済みパラメーターファイル名\n",
    "prediction = Prediction(input_dim, output_dim)\n",
    "row = t_train.shape[0]\n",
    "\n",
    "t_one_hot = np.zeros((row, input_dim), dtype='int8')    # ラベルデータをN-hot化\n",
    "for i in range(0, row) :\n",
    "    for j in range(0, maxlen*2):\n",
    "        t_one_hot[i, t_train[i,j]] = 1\n",
    "\n",
    "x_train = x_train.reshape(row,1)\n",
    "model = prediction.train(x_train, t_one_hot, batch_size, epochs, maxlen, emb_param)\n",
    "\n",
    "model.save_weights(emb_param)           # 学習済みパラメーターセーブ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 埋め込みベクトルの評価\n",
    "\n",
    "\n",
    "param_lstm = model.get_weights()\n",
    "param = param_lstm[0]\n",
    "word0 = '一'\n",
    "word1 = '１'\n",
    "word2 = '２'\n",
    "vec0 = param[word_indices[word0],:]\n",
    "vec1 = param[word_indices[word1],:]\n",
    "vec2 = param[word_indices[word2],:]\n",
    "\n",
    "vec = vec0 - vec1 + vec2\n",
    "vec_norm = math.sqrt(np.dot(vec, vec))\n",
    "\n",
    "w_list = [word_indices[word0], word_indices[word1], word_indices[word2]]\n",
    "dist = -1.0\n",
    "m = 0\n",
    "for j in range(0, 5) :\n",
    "    dist = -1.0\n",
    "    m = 0\n",
    "    for i in range(0, len(words)) :\n",
    "        if i not in w_list :\n",
    "            dist0 = np.dot(vec, param[i,:])\n",
    "            dist0 = dist0 / vec_norm / math.sqrt(np.dot(param[i,:], param[i,:]))\n",
    "            if dist < dist0 :\n",
    "                dist = dist0\n",
    "                m = i\n",
    "    print('第' + str(j+1) + '候補:')\n",
    "    print('コサイン類似度=', dist, ' ', m, ' ', indices_word[m])\n",
    "    w_list.append(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
