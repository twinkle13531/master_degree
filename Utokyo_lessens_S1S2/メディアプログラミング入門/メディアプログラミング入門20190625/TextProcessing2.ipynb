{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理２：言語モデル\n",
    "\n",
    "Python入門では、第７回で文字n-gramを生成し、著者が分からない小説が太宰治のものか宮沢賢治のものかを当てるというミニプロジェクトをやってもらいました。   \n",
    "今回は、形態素解析結果を用いて、単語n-gramを使った自然言語処理について勉強しましょう。\n",
    "\n",
    "ここで、Janomeで生成した分かち書き文は単語区切りではなく形態素区切りですが、   \n",
    "必ずしも言語学的な形態素とは言えない（単語と呼ぶべき）用語も交じっており、   \n",
    "また、形態素区切りを単語区切りに変換するのは言語学的知識が必要となるので、   \n",
    "ここではJanomeが出力した形態素を単語と呼び変えることにします。\n",
    "\n",
    "本教材の作品データは[青空文庫](https://www.aozora.gr.jp/index.html)のものを使用しています。   \n",
    "ただし、ルビや入力者注、アクセント分解された欧文や編者による注記等は削除しました。   \n",
    "また、詩のように短い文章から構成されるものをのぞくなど、調整を行っています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 準備：分かち書き\n",
    "\n",
    "言語モデルを学習する前に、解析したい文を分かち書きしましょう。\n",
    "\n",
    "文章を分かち書きに変換するツールは前の資料で紹介しました。   \n",
    "ここでは、宮沢賢治の作品147品の本文をjanomeを使って分かち書きし、１つのファイルにまとめた`miyazawa_wakati.txt`を使って処理を行います。   \n",
    "（このファイルの作り方はわかりますね？本当は皆さんにやっていただきたいのですが、全147作品の形態素解析はそこそこ時間がかかります。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27262\n",
      "[['<BOP>', 'その', '明け方', 'の', '空', 'の', '下', '、', 'ひる', 'の', '鳥', 'で', 'も', 'ゆか', 'ない', '高い', 'ところ', 'を', 'するどい', '霜', 'の', 'かけ', 'ら', 'が', '風', 'に', '流さ', 'れ', 'て', 'サラサラ', 'サラサラ', '南', 'の', 'ほう', 'へ', 'とん', 'で', 'ゆき', 'まし', 'た', '。', '<EOP>'], ['<BOP>', 'じつに', 'その', 'かすか', 'な', '音', 'が', '丘', 'の', '上', 'の', '一', '本', 'いちょう', 'の', '木', 'に', '聞こえる', 'くらい', 'すみきっ', 'た', '明け方', 'です', '。', '<EOP>']]\n"
     ]
    }
   ],
   "source": [
    "file = 'text/miyazawa_wakati.txt'\n",
    "\n",
    "words = [('<BOP> ' + l + ' <EOP>').split() for l in open(file, 'r', encoding='utf-8').readlines()]\n",
    "\n",
    "# 先頭の2文分を出力してみましょう\n",
    "print(len(words))\n",
    "print(words[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. n-gram生成\n",
    "### 2.1 自然言語処理用ライブラリNLTKの利用\n",
    "\n",
    "n = 1のときuni-gram, n = 2のときbi-gram、n = 3のときtri-gramと呼びます。   \n",
    "宮沢作品集の分かち書きが入っている`words`から、uni-gram, bi-gram, tri-gramをそれぞれ計算してみましょう。   \n",
    "ここではNLTKのn-gram生成モジュールであるngramsを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram [('<BOP>',), ('その',), ('明け方',), ('の',), ('空',), ('の',), ('下',), ('、',), ('ひる',), ('の',), ('鳥',), ('で',), ('も',), ('ゆか',), ('ない',), ('高い',), ('ところ',), ('を',), ('するどい',), ('霜',), ('の',), ('かけ',), ('ら',), ('が',), ('風',), ('に',), ('流さ',), ('れ',), ('て',), ('サラサラ',), ('サラサラ',), ('南',), ('の',), ('ほう',), ('へ',), ('とん',), ('で',), ('ゆき',), ('まし',), ('た',), ('。',), ('<EOP>',)]\n",
      "bigram [('<BOP>', 'その'), ('その', '明け方'), ('明け方', 'の'), ('の', '空'), ('空', 'の'), ('の', '下'), ('下', '、'), ('、', 'ひる'), ('ひる', 'の'), ('の', '鳥'), ('鳥', 'で'), ('で', 'も'), ('も', 'ゆか'), ('ゆか', 'ない'), ('ない', '高い'), ('高い', 'ところ'), ('ところ', 'を'), ('を', 'するどい'), ('するどい', '霜'), ('霜', 'の'), ('の', 'かけ'), ('かけ', 'ら'), ('ら', 'が'), ('が', '風'), ('風', 'に'), ('に', '流さ'), ('流さ', 'れ'), ('れ', 'て'), ('て', 'サラサラ'), ('サラサラ', 'サラサラ'), ('サラサラ', '南'), ('南', 'の'), ('の', 'ほう'), ('ほう', 'へ'), ('へ', 'とん'), ('とん', 'で'), ('で', 'ゆき'), ('ゆき', 'まし'), ('まし', 'た'), ('た', '。'), ('。', '<EOP>')]\n",
      "trigram [('<BOP>', 'その', '明け方'), ('その', '明け方', 'の'), ('明け方', 'の', '空'), ('の', '空', 'の'), ('空', 'の', '下'), ('の', '下', '、'), ('下', '、', 'ひる'), ('、', 'ひる', 'の'), ('ひる', 'の', '鳥'), ('の', '鳥', 'で'), ('鳥', 'で', 'も'), ('で', 'も', 'ゆか'), ('も', 'ゆか', 'ない'), ('ゆか', 'ない', '高い'), ('ない', '高い', 'ところ'), ('高い', 'ところ', 'を'), ('ところ', 'を', 'するどい'), ('を', 'するどい', '霜'), ('するどい', '霜', 'の'), ('霜', 'の', 'かけ'), ('の', 'かけ', 'ら'), ('かけ', 'ら', 'が'), ('ら', 'が', '風'), ('が', '風', 'に'), ('風', 'に', '流さ'), ('に', '流さ', 'れ'), ('流さ', 'れ', 'て'), ('れ', 'て', 'サラサラ'), ('て', 'サラサラ', 'サラサラ'), ('サラサラ', 'サラサラ', '南'), ('サラサラ', '南', 'の'), ('南', 'の', 'ほう'), ('の', 'ほう', 'へ'), ('ほう', 'へ', 'とん'), ('へ', 'とん', 'で'), ('とん', 'で', 'ゆき'), ('で', 'ゆき', 'まし'), ('ゆき', 'まし', 'た'), ('まし', 'た', '。'), ('た', '。', '<EOP>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text_unigrams = [ngrams(word, 1) for word in words] # uni-gramを計算\n",
    "text_bigrams = [ngrams(word, 2) for word in words] # bi-gramを計算\n",
    "text_trigrams = [ngrams(word, 3) for word in words] # tri-gramを計算\n",
    "\n",
    "# 最初の1文のn-gramを出力してみましょう\n",
    "print('unigram', [x for x in text_unigrams[0]])\n",
    "print('bigram',[x for x in text_bigrams[0]])\n",
    "print('trigram',[x for x in text_trigrams[0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 n-gramの出現回数\n",
    "\n",
    "nltk.NgramCounterを使って各n-gramが宮沢作品集にそれぞれいくつずつ登場するか数えてみましょう。   \n",
    "まずは下準備です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk.lm import NgramCounter\n",
    "\n",
    "# じつはngramsが返すのはイテレータなので何度も使いまわせないことに注意！\n",
    "text_unigrams = [ngrams(word, 1) for word in words] # uni-gramを計算\n",
    "text_bigrams = [ngrams(word, 2) for word in words] # bi-gramを計算\n",
    "text_trigrams = [ngrams(word, 3) for word in words] # tri-gramを計算\n",
    "\n",
    "# uni-gram, bi-gram, tri-gramをまとめて一気に出現回数を数える\n",
    "ngram_counts = NgramCounter(text_unigrams + text_bigrams +text_trigrams) # N-gramの出現回数をカウント\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で計算した結果を使ってさまざまなn-gramの出現回数を表示してみよう。   \n",
    "\n",
    "ここで、\n",
    "- `ngram_counts['ジヨバンニ'])`には「ジヨバンニ」の出現回数\n",
    "- `ngram_counts[['ジヨバンニ']]`には、「ジヨバンニ」に続く単語の出現回数\n",
    "- `ngram_counts[['ジヨバンニ','は']]`には、「ジヨバンニ は」という２つの単語の後に続く単語の出現回数\n",
    "\n",
    "が記録されていることに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ジヨバンニ: 205\n",
      "ジヨバンニ\n",
      "->\tは: 123\n",
      "->\tが: 27\n",
      "->\tの: 21\n",
      "->\tも: 10\n",
      "->\tに: 6\n",
      "->\t、: 5\n",
      "->\tさん: 4\n",
      "->\tを: 3\n",
      "->\tたち: 3\n",
      "->\tと: 1\n",
      "->\tや: 1\n",
      "->\tまで: 1\n",
      "\n",
      "ジヨバンニ は\n",
      "->\t、: 33\n",
      "->\t思は: 7\n",
      "->\tまるで: 5\n",
      "->\t思ひ: 5\n",
      "->\tまつ: 5\n",
      "->\tもう: 4\n",
      "->\tすぐ: 2\n",
      "->\tその: 2\n",
      "->\t何: 2\n",
      "->\t俄: 2\n",
      "->\t窓: 2\n",
      "->\t橋: 2\n",
      "->\tなんだか: 2\n",
      "->\t云: 2\n",
      "->\tまた: 2\n",
      "->\t勢: 1\n",
      "->\t手: 1\n",
      "->\t拾: 1\n",
      "->\tおじぎ: 1\n",
      "->\t靴: 1\n",
      "->\t玄: 1\n",
      "->\t立つ: 1\n",
      "->\t高く: 1\n",
      "->\tわれ: 1\n",
      "->\t帽子: 1\n",
      "->\tせ: 1\n",
      "->\tなんとも: 1\n",
      "->\t走り: 1\n",
      "->\tぢ: 1\n",
      "->\t町: 1\n",
      "->\t眼: 1\n",
      "->\t一: 1\n",
      "->\t叫び: 1\n",
      "->\tまだ: 1\n",
      "->\tなぜ: 1\n",
      "->\tどんどん: 1\n",
      "->\tいきなり: 1\n",
      "->\tみんな: 1\n",
      "->\tわくわく: 1\n",
      "->\tかすか: 1\n",
      "->\tそれ: 1\n",
      "->\t（: 1\n",
      "->\t胸: 1\n",
      "->\tびつくり: 1\n",
      "->\t困: 1\n",
      "->\tたしかに: 1\n",
      "->\tこんな: 1\n",
      "->\t首: 1\n",
      "->\t坊: 1\n",
      "->\t川下: 1\n",
      "->\t生: 1\n",
      "->\t熱: 1\n",
      "->\tどうしても: 1\n",
      "->\tだんだん: 1\n",
      "->\tもうす: 1\n",
      "->\tあぶなく: 1\n",
      "->\tそつ: 1\n",
      "->\t自分: 1\n",
      "->\t唇: 1\n",
      "->\t力強く: 1\n",
      "->\t叫ん: 1\n"
     ]
    }
   ],
   "source": [
    "# 「ジヨバンニ」のuni-gram出現回数\n",
    "print('ジヨバンニ: ' + str(ngram_counts['ジヨバンニ']))\n",
    "\n",
    "# 「ジヨバンニ」に続く単語の出現回数（bi-gram出現回数）\n",
    "print('ジヨバンニ')\n",
    "for word, count in sorted(ngram_counts[['ジヨバンニ']].items(), key=lambda x:x[1], reverse=True):\n",
    "    print('->\\t{:s}: {:d}'.format(word, count))\n",
    "print()\n",
    "\n",
    "# 「ジヨバンニ は」に続く単語の出現回数（tri-gram出現回数）\n",
    "print('ジヨバンニ は')\n",
    "for word, count in sorted(ngram_counts[['ジヨバンニ','は']].items(), key=lambda x:x[1], reverse=True):\n",
    "    print('->\\t{:s}: {:d}'.format(word, count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 言語モデルの学習\n",
    "\n",
    "n-gram確率とは、その文書において、直前の$(n-1)$単語が与えられたとき、その後に続く単語の出現確率を表しています。   \n",
    "たとえば上の例では、「ジヨバンニ は」という２単語が現れたとき、その後に続くのは「、」が３３回です。   \n",
    "「ジヨバンニ は」の出現回数は123回なので、「ジヨバンニ は」の後に「、」が続く確率（tri-gram確率）は$33/123=0.268293$ということになります。   \n",
    "これをすべてのn-gramについて計算することを、ここでは「学習」と呼びます。\n",
    "\n",
    "tri-gram確率を最尤推定法（Maximum Likelihood Estimation)により学習します。\n",
    "これには少し時間がかかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22793\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import Vocabulary\n",
    "from nltk.lm.models import MLE\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# 読み込んだ小説集の語彙（異なり単語）を収集\n",
    "# Vocabularyは1次元のリストを受け取るが、wordsは2次元のリストなので、\n",
    "# wordsを内包表記で2次元から1次元に変換してからVocabularyに渡しています\n",
    "vocab = Vocabulary([item for sublist in words for item in sublist])\n",
    "\n",
    "# 語彙の一覧を表示させたいなら下4行のコメントを有効にする\n",
    "'''num = 0\n",
    "for v in sorted(vocab.counts):\n",
    "    print('{:d}\\t{:s}'.format(num,v))\n",
    "    num += 1\n",
    "'''\n",
    "print('Vocabulary size: ' + str(len(vocab))) # 語彙サイズ（単語の種類数）\n",
    "\n",
    "text_trigrams = [ngrams(word, 3) for word in words] # tri-gramを生成\n",
    "\n",
    "n = 3\n",
    "lm = MLE(order = n, vocabulary = vocab) # 最尤推定法（Maximum Likelihood Estimation)による統計的n-gram言語モデルの準備\n",
    "lm.fit(text_trigrams) # 上で生成したtri-gramを使って言語モデルを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 統計的n-gramの活用\n",
    "### 4.1 指定された単語列に続く単語を調べよう\n",
    "\n",
    "宮沢作品において、「ジヨバンニは」と来たら次にはどんな単語が続くでしょうか？    \n",
    "（「ジヨバンニ は」の後に「、」が続く確率は先ほど計算しましたね。0.268293でした）。  \n",
    "その確率を計算してみましょう。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ジヨバンニ', 'は']\n",
      "\t、: 0.268293\n",
      "\t思は: 0.056911\n",
      "\tまるで: 0.040650\n",
      "\t思ひ: 0.040650\n",
      "\tまつ: 0.040650\n",
      "\tもう: 0.032520\n",
      "\tすぐ: 0.016260\n",
      "\tその: 0.016260\n",
      "\t何: 0.016260\n",
      "\t俄: 0.016260\n",
      "\t窓: 0.016260\n",
      "\t橋: 0.016260\n",
      "\tなんだか: 0.016260\n",
      "\t云: 0.016260\n",
      "\tまた: 0.016260\n",
      "\t勢: 0.008130\n",
      "\t手: 0.008130\n",
      "\t拾: 0.008130\n",
      "\tおじぎ: 0.008130\n",
      "\t靴: 0.008130\n",
      "\t玄: 0.008130\n",
      "\t立つ: 0.008130\n",
      "\t高く: 0.008130\n",
      "\tわれ: 0.008130\n",
      "\t帽子: 0.008130\n",
      "\tせ: 0.008130\n",
      "\tなんとも: 0.008130\n",
      "\t走り: 0.008130\n",
      "\tぢ: 0.008130\n",
      "\t町: 0.008130\n",
      "\t眼: 0.008130\n",
      "\t一: 0.008130\n",
      "\t叫び: 0.008130\n",
      "\tまだ: 0.008130\n",
      "\tなぜ: 0.008130\n",
      "\tどんどん: 0.008130\n",
      "\tいきなり: 0.008130\n",
      "\tみんな: 0.008130\n",
      "\tわくわく: 0.008130\n",
      "\tかすか: 0.008130\n",
      "\tそれ: 0.008130\n",
      "\t（: 0.008130\n",
      "\t胸: 0.008130\n",
      "\tびつくり: 0.008130\n",
      "\t困: 0.008130\n",
      "\tたしかに: 0.008130\n",
      "\tこんな: 0.008130\n",
      "\t首: 0.008130\n",
      "\t坊: 0.008130\n",
      "\t川下: 0.008130\n",
      "\t生: 0.008130\n",
      "\t熱: 0.008130\n",
      "\tどうしても: 0.008130\n",
      "\tだんだん: 0.008130\n",
      "\tもうす: 0.008130\n",
      "\tあぶなく: 0.008130\n",
      "\tそつ: 0.008130\n",
      "\t自分: 0.008130\n",
      "\t唇: 0.008130\n",
      "\t力強く: 0.008130\n",
      "\t叫ん: 0.008130\n",
      "ある単語列に続くtri-gram確率をすべて足したら 1.0000000000000018\n"
     ]
    }
   ],
   "source": [
    "context = ['ジヨバンニ', 'は']\n",
    "#context = ['カムパネルラ', 'は']\n",
    "print(context)\n",
    "\n",
    "# contextに続く単語のリストを獲得\n",
    "# lm.vocab.lookup(context)は、contextに含まれる単語のうち、vocabに含まれていない\n",
    "# 単語があったとき、その単語を<UNK>に置き換える\n",
    "# （UNKはUn-knownのこと。日本語では未知語と呼ぶ）\n",
    "prob_list = [(word, lm.score(word, context)) for word\n",
    "            in lm.context_counts(lm.vocab.lookup(context))]\n",
    "prob_list.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sum_prob = 0.0 # 「ジョバンニは」に続く単語のtri-gram確率をすべて足すとどうなるでしょう？\n",
    "for word, prob in prob_list:\n",
    "    print('\\t{:s}: {:f}'.format(word, prob))\n",
    "    sum_prob += prob\n",
    "    \n",
    "print('ある単語列に続くtri-gram確率をすべて足したら', sum_prob) \n",
    "# 結果は1.0になります（誤差があります）\n",
    "# このtri-gramが、『「ジヨバンニは」という単語列に続く』という条件のもと、\n",
    "# それに続く単語の出現確率（つまり条件付き確率）になっていることが分かりますね。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはn-gram確率です。上でNgramCounterをつかって計算したn-gram出現回数と結果を比べてみてください。   \n",
    "NgramCounterで数えたのは「回数」ですが、lm(言語モデル）では「確率」になっています。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ランダム文生成\n",
    "\n",
    "「ジヨバンニは」に続く文を適当に生成してみましょう。実際にあまり使いどころはないですが、今回の課題の一つですのでやってみてください。  \n",
    "\n",
    "すでにtri-gram確率を計算しているので、ここでは、直前の2単語だけをみて、次に続く単語の出現確率が0じゃないものをランダムに選んでつないでいきます。   \n",
    "まさしく人工無能の真骨頂です。\n",
    "\n",
    "関数lm.generateは、text=seedで指定された単語列を直前の文として、次に続く単語をランダムに選びます。   \n",
    "このとき、その単語が選ばれる確率は、そのn-gram確率に従います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ジヨバンニ', 'は']\n",
      "['ジヨバンニ', 'は', '、']\n",
      "['ジヨバンニ', 'は', '、', '大きく']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う', 'の']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う', 'の', '渚']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う', 'の', '渚', 'による']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う', 'の', '渚', 'による', '沙']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う', 'の', '渚', 'による', '沙', 'に']\n",
      "['ジヨバンニ', 'は', '、', '大きく', '口', 'を', 'あか', 'なかっ', 'た', '薬師', '岳', 'と', 'の', 'なか', 'に', '「', '及', '」', 'とか', '、', '全く', '向う', 'の', '渚', 'による', '沙', 'に', '<EOP>']\n"
     ]
    }
   ],
   "source": [
    "### ランダム文生成 ####\n",
    "# contextから始まる文を生成\n",
    "\n",
    "# 最初の2単語はいろいろと変えてみましょう\n",
    "context = ['ジヨバンニ', 'は']\n",
    "#context = ['カムパネルラ', 'は']\n",
    "print(context)\n",
    "\n",
    "for i in range(0, 100):\n",
    "    # contextのうち最後の2単語から次に繋がる確率0じゃない単語をランダムに選ぶ\n",
    "    w = lm.generate(text_seed=context)\n",
    "    context.append(w) # 選ばれた単語をcontextに連結\n",
    "    print(context)\n",
    "    \n",
    "    if '。' == w or '<EOP>' == w: # 句点「。」か<EOP>に到達したらそこで終了\n",
    "        break\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 文の生起確率$P(W)$の算出\n",
    "\n",
    "宮沢賢治の小説集をコーパスとして学習した統計的tri-gramを使って、与えられた文（単語列）がどの程度『宮沢賢治らしい』かを推定してみましょう。   \n",
    "これは、n-gram確率を\n",
    "\n",
    "$$p(w_i|w_{i-n+1},...,w_{i-1})$$\n",
    "\n",
    "としたとき、入力文$W=({w_0, w_1, ..., w_n})$に対して、文の生起確率\n",
    "\n",
    "$$P(W)=\\Pi_{i=0}^{n+1}p(w_i|w_{i-n+1},...,w_{i-1})$$\n",
    "\n",
    "の計算をすることを意味しています(ただし、$w_i$はその文の$i$番目の単語を表し、$w_0$のとき文頭、$w_{n+1}$のとき文末記号を表す）。   \n",
    "詳しくは授業でスライドを使って説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ジヨバンニ', 'は') 何 :\t 0.016260162601626018\n",
      "\n",
      "('は', '何') げ :\t 0.006493506493506494\n",
      "\n",
      "('何', 'げ') なく :\t 1.0\n",
      "\n",
      "('げ', 'なく') 答え :\t 0.3333333333333333\n",
      "\n",
      "('なく', '答え') まし :\t 1.0\n",
      "\n",
      "('答え', 'まし') た :\t 1.0\n",
      "\n",
      "('まし', 'た') 。 :\t 0.8878514702725907\n",
      "\n",
      "3.124807201888539e-05\n"
     ]
    }
   ],
   "source": [
    "### 入力文がどの程度宮沢賢治らしい文章かを判定してみよう ###\n",
    "\n",
    "# どちらの文のほうが生起確率が高いでしょうか？構成する単語はどちらも同じです。\n",
    "line = 'ジヨバンニ は 何 げ なく 答え まし た 。'\n",
    "#line = '何 げ なく ジヨバンニ は 答え まし た 。'\n",
    "\n",
    "words2 = line.split() # 空白で区切ってリストに代入します\n",
    "\n",
    "n = 3 # lm (言語モデル) はtri-gramで学習しているので、ここではn=3を指定\n",
    "probability = 1.0 # 始め確率は1.0に初期化\n",
    "for ngram in ngrams(words2, n):\n",
    "    # 2単語の後に3単語目が続く確率をひたすら計算\n",
    "    prob = lm.score(lm.vocab.lookup(ngram[-1]),\n",
    "                    lm.vocab.lookup(ngram[:-1]))\n",
    "    print(ngram[:-1], ngram[-1], ':\\t', prob)\n",
    "    print()\n",
    "    \n",
    "    # 確率が0(すなわち、そのような文字の連結は宮沢作品に一度も現れない)の場合は\n",
    "    # そのtri-gramの生起確率は0になってしまう。それをProbabilityに掛けると、\n",
    "    # たとえ他のtri-gramが頻出するものであっても0になるので、\n",
    "    # ここでは0のときは微小な値をprobに代入する\n",
    "    prob = max(prob, 1e-8)\n",
    "    \n",
    "    # tri-gramの生起確率をかけ合わせていく\n",
    "    probability *= prob\n",
    "    \n",
    "print(probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 「何 げ なく ジヨバンニ は 答え まし た 。」の出現確率は6.658886027044431e-25\n",
    "- 「ジヨバンニ は 何 げ なく 答え まし た 。」の出現確率は3.124807201888539e-05\n",
    "\n",
    "ということで、後者の方がそれらしい文であることがわかります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
