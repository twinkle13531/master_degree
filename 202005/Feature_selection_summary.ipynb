{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.codexa.net/feature-selection-methods/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continuousな特徴量で、categoricalな目的変数のときの特徴量選択\n",
    "- Filter method...LDA(フィッシャーのLDA, linear discriminant analysis)\n",
    "- Wrapper Method...Forward Selection, Backward Elimination, Recursive Feature Elimination\n",
    "- Embedded Method...Lasso回帰, Ridge回帰, Regularized trees, Memetic algorithm, Random multinomial logit\n",
    "  - 決定木で特徴の重要度を抽出できたり、線形判別なら係数が直接特徴の重要度となるので、これを用いて変数選択もできそうです。 また、この方法のことをモデルベース特徴量選択というみたいです。\n",
    "\n",
    "\n",
    "\n",
    "## Filter Method\n",
    "- 単変量統計 (univariance statistics)\n",
    "- 統計のテクニックを用いて各特徴の「予測に使える度合」を点数化します。点数をもとに特徴にランク付けを行い、予測に使うか否かをそれぞれ決定していきます。統計的手法で特徴をの有用性を評価する\n",
    "- 説明変数と目的変数の関係性から変数選択をする。\n",
    "- 早い\n",
    "- 最適な特徴の部分集合を見つけられない\n",
    "- カイ二乗検定（Chi-Square), ANOVA（Analytics of Variance）\n",
    "\n",
    "\n",
    "## Wrapper Method\n",
    "- 反復選択 (iterative selection)\n",
    "- 複数の特徴を同時に使って予測精度の検証を行い、精度が最も高くなるような特徴量の組み合わせを探索していきます。様々な組み合わせでそれぞれ学習を行わせ、その学習結果をもとに組み合わせに優劣をつけていきます。\n",
    "- 計算量が多い、超遅い\n",
    "- 交差検証を使用する。(Wrapper Methodを使うときは交差検証しろよ！ということかなと思いました。)\n",
    "- 結構良い特徴の部分集合が見つかる。\n",
    "- 選んだ特徴量が過学習する\n",
    "  - 前進法 (Forward selection)\n",
    "  - 後退法 (Backward elimination)\n",
    "  - Recursive Feature Elimination (RFE)\n",
    "\n",
    "\n",
    "## Embedded Method\n",
    "- モデルベース特徴量選択（model-based selection）\n",
    "- 学習アルゴリズムに特徴量選択が組み込まれているので、学習と特徴量選択を同時に行うことができます。\n",
    "- ラッソ回帰（LASSO Linear Regression）, 決定木 (決定木ベースモデルのfeature_importances_), 線形回帰におけるL1正則化\n",
    "- lasso\n",
    "  - 選んだ特徴量が過学習する\n",
    "  - ロジスティクス回帰やSVMで用いたことしかないので、めっちゃ効く手法もあるか\n",
    "- ランダムフォレスト\n",
    "  - どれぐらいの特徴量重要度があったら重要だと言えるのかがイマイチ\n",
    "  - ランダム性から訓練するたびに特徴量重要度が変動する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴量選択とコード\n",
    "- その一　Filter method\n",
    "https://aotamasaki.hatenablog.com/entry/2018/04/27/222536\n",
    "\n",
    "```\n",
    "#mask_oriの計算（以降のエネルギー計算の基準となる）\n",
    "#ori_y == yとなる場合だけ、E(y)==0となる仕様\n",
    "\n",
    "selector = SelectKBest(k=num_select) #Filter method #単変量選択（分類：chi2、f_classif、mutual_info_classif）\n",
    "selector.fit(X, series_y)\n",
    "mask_ori = selector.get_support() #各特徴量を選択したか否かのmaskを取得\n",
    "```\n",
    "\n",
    "- その二　Wrapper method\n",
    "```\n",
    "#recursive feature eliminationを用いた選択\n",
    "#スコア算出ができるので特徴量数を指定できるかもしれない\n",
    "select = RFECV(LogisticRegression(), cv=10, scoring='average_precision')\n",
    "select.fit(X, y)\n",
    "mask = select.support_\n",
    "```\n",
    "\n",
    "- その三　Boruta：randomForestを用いた変数選択法\n",
    "https://www.google.co.jp/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0ahUKEwiEzMP4p9naAhWBk5QKHbRjC9oQFggoMAA&url=https%3A%2F%2Fwww.jstatsoft.org%2Farticle%2Fview%2Fv036i11%2Fv36i11.pdf&usg=AOvVaw3tyiHN0BCe2fkkAA6xEVDE\n",
    "\n",
    "```\n",
    "forest = RandomForestClassifier()\n",
    "# define Boruta feature selection method\n",
    "select = boruta_py.BorutaPy(forest, n_estimators=10)\n",
    "select.fit(X, y)\n",
    "mask = select.support_\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
